{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание №7\n",
    "\n",
    "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), @neychev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача №1: \n",
    "Обратимся к классической задаче распознавания рукописных цифр. Мы будем работать с набором данных [MNIST](http://yann.lecun.com/exdb/mnist/). В данном задании воспользуемся всем датасетом целиком.\n",
    "\n",
    "__Ваша основная задача: реализовать весь пайплан обучения модели и добиться качества $\\geq 92\\%$ на тестовой выборке.__\n",
    "\n",
    "Код для обучения модели в данном задании отсутствует. Присутствует лишь несколько тестов, которые помогут вам отладить свое решение. За примером можно обратиться к ноутбуку первого занятия.\n",
    "\n",
    "Настоятельно рекомендуем написать код \"с нуля\", лишь поглядывая на готовые примеры, а не просто \"скопировать-вставить\". Это поможет вам в дальнейшем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to .\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:10<00:00, 946563.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\train-images-idx3-ubyte.gz to .\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to .\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 6583819.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\train-labels-idx1-ubyte.gz to .\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to .\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:03<00:00, 496962.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to .\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to .\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 459170.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to .\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Image label: 3')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "train_mnist_data = MNIST('.', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test_mnist_data = MNIST('.', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_mnist_data,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_mnist_data,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "random_batch = next(iter(train_data_loader))\n",
    "_image, _label = random_batch[0][0], random_batch[1][0]\n",
    "plt.figure()\n",
    "plt.imshow(_image.reshape(28, 28))\n",
    "plt.title(f'Image label: {_label}')\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постройте модель ниже. Пожалуйста, не стройте переусложненную сеть, не стоит делать ее глубже четырех слоев (можно и меньше). Ваша основная задача – обучить модель и получить качество на отложенной (тестовой выборке) не менее 92% accuracy.\n",
    "\n",
    "*Комментарий: для этого достаточно линейных слоев и функций активации.*\n",
    "\n",
    "__Внимание, ваша модель должна быть представлена именно переменной `model`.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model instance\n",
    "\n",
    "\n",
    "first_out_number = 1000\n",
    "second_out_number = 1000\n",
    "\n",
    "model = torch.nn.Sequential( # your code here\n",
    "    torch.nn.Linear(28*28, first_out_number),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(first_out_number, second_out_number),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(second_out_number, 10),\n",
    "    torch.nn.Sigmoid()\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Локальные тесты для проверки вашей модели доступны ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything seems fine!\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert model is not None, 'Please, use `model` variable to store your model'\n",
    "\n",
    "try:\n",
    "    x = random_batch[0].reshape(-1, 784)\n",
    "    y = random_batch[1]\n",
    "\n",
    "    # compute outputs given inputs, both are variables\n",
    "    y_predicted = model(x)    \n",
    "except Exception as e:\n",
    "    print('Something is wrong with the model')\n",
    "    raise e\n",
    "    \n",
    "    \n",
    "assert y_predicted.shape[-1] == 10, 'Model should predict 10 logits/probas'\n",
    "\n",
    "print('Everything seems fine!')\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настройте параметры модели на обучающей выборке. Рекомендуем поработать с различными оптимизаторами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3004, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0389, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1331, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1179, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1139, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1110, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1938, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss_func(model(xb), yb))\n\u001b[0;32m     14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss_func(model(xb), yb))\n",
      "File \u001b[1;32mc:\\Users\\nik1m\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nik1m\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\nik1m\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     adam(\n\u001b[0;32m    167\u001b[0m         params_with_grad,\n\u001b[0;32m    168\u001b[0m         grads,\n\u001b[0;32m    169\u001b[0m         exp_avgs,\n\u001b[0;32m    170\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    171\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    172\u001b[0m         state_steps,\n\u001b[0;32m    173\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    174\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    175\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    176\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    177\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    178\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    179\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    180\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    181\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    182\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    183\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    184\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    185\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    186\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    187\u001b[0m     )\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\nik1m\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m func(params,\n\u001b[0;32m    317\u001b[0m      grads,\n\u001b[0;32m    318\u001b[0m      exp_avgs,\n\u001b[0;32m    319\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    320\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    321\u001b[0m      state_steps,\n\u001b[0;32m    322\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m    323\u001b[0m      has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    324\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    325\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    326\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    327\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m    328\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    329\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m    330\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m    331\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m    332\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[0;32m    333\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\nik1m\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    388\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 391\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m    392\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "xb = x\n",
    "yb = y\n",
    "\n",
    "\n",
    "for epoch in range(1000):\n",
    "    pred = model(x)\n",
    "    loss = loss_func(pred, yb)\n",
    "\n",
    "    print(loss_func(model(xb), yb))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также, напоминаем, что в любой момент можно обратиться к замечательной [документации](https://pytorch.org/docs/stable/index.html) и [обучающим примерам](https://pytorch.org/tutorials/).  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим качество классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = []\n",
    "real_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in train_data_loader:\n",
    "        y_predicted = model(batch[0].reshape(-1, 784))\n",
    "        predicted_labels.append(y_predicted.argmax(dim=1))\n",
    "        real_labels.append(batch[1])\n",
    "\n",
    "predicted_labels = torch.cat(predicted_labels)\n",
    "real_labels = torch.cat(real_labels)\n",
    "train_acc = (predicted_labels == real_labels).type(torch.FloatTensor).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on train set: 0.27077\n"
     ]
    }
   ],
   "source": [
    "print(f'Neural network accuracy on train set: {train_acc:3.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = []\n",
    "real_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_data_loader:\n",
    "        y_predicted = model(batch[0].reshape(-1, 784))\n",
    "        predicted_labels.append(y_predicted.argmax(dim=1))\n",
    "        real_labels.append(batch[1])\n",
    "\n",
    "predicted_labels = torch.cat(predicted_labels)\n",
    "real_labels = torch.cat(real_labels)\n",
    "test_acc = (predicted_labels == real_labels).type(torch.FloatTensor).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Neural network accuracy on test set: {test_acc:3.5}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка, что необходимые пороги пройдены:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_acc >= 0.92, 'Test accuracy is below 0.92 threshold'\n",
    "assert train_acc >= 0.91, 'Train accuracy is below 0.91 while test accuracy is fine. We recommend to check your model and data flow'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сдача задания\n",
    "Загрузите файл `hw07_data_dict.npy` (ссылка есть на странице с заданием) и запустите код ниже для генерации посылки. Код ниже может его загрузить (но в случае возникновения ошибки скачайте и загрузите его вручную)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/23s_dd_ml/homeworks/hw07_mnist_classification/hw07_data_dict.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import os\n",
    "\n",
    "assert os.path.exists('hw07_data_dict.npy'), 'Please, download `hw07_data_dict.npy` and place it in the working directory'\n",
    "\n",
    "def get_predictions(model, eval_data, step=10):\n",
    "    \n",
    "    predicted_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, len(eval_data), step):\n",
    "            y_predicted = model(eval_data[idx:idx+step].reshape(-1, 784))\n",
    "            predicted_labels.append(y_predicted.argmax(dim=1))\n",
    "    \n",
    "    predicted_labels = torch.cat(predicted_labels)\n",
    "    return predicted_labels\n",
    "\n",
    "loaded_data_dict = np.load('hw07_data_dict.npy', allow_pickle=True)\n",
    "\n",
    "submission_dict = {\n",
    "    'train': get_predictions(model, torch.FloatTensor(loaded_data_dict.item()['train'])).numpy(),\n",
    "    'test': get_predictions(model, torch.FloatTensor(loaded_data_dict.item()['test'])).numpy()\n",
    "}\n",
    "\n",
    "np.save('submission_dict_hw07.npy', submission_dict, allow_pickle=True)\n",
    "print('File saved to `submission_dict_hw07.npy`')\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом задание завершено. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
